The intraparietal sulcus is a large and heterogeneous area in the monkey, and at least five functionally- and neuroanatomically-distinct sub regions are found here (Colby & Duhamel, 1991; Rizzolatti et al , 1998, see Figure 1), with a variety of neuronal response properties ranging from purely somatosensory, to purely visual. Even then, the influence of attention and response preparation may be serious confounding factors. Now, if peripersonal space is coded in terms of a bimodal, visuotactile representation, and if a right hemisphere lesion affects this representation, then peripersonal visual stimuli on the right should interfere with the detection of tactile stimuli on the left. When the right visual stimulus was presented far from the hand, or when the patient held their own hand behind their back (see Figure 3), detection of left tactile stimuli improved significantly. Furthermore, when the arms were held in a crossed position (such that the left hand lay in the right hemispace and vice versa for the right hand), visual stimulation near the right hand still induced extinction of left hand tactile stimuli. Stimulation of the same point in space near the hand, when the hand is occluded from view, produces extinction only to the same degree as for visual stimuli presented far from the right hand (e.g., Mattingley et al , 1997).3 Thus, it would appear that proprioceptive signals are insufficient to allow a visual peripersonal stimulus to activate strongly the visuotactile representation of the hand. In summary, combined tactile stimulation of a body part and visual stimulation near that same body part can ameliorate both tactile and visual perceptual deficits. Third, not only arm-centred, but also face-centred, deficits in peripersonal space have been documented. In their patient C.A., right auditory stimulation interfered with the detection of a simultaneous left tactile stimulus (the experimenter tapped the patient’s skin behind her left ear) much more when auditory stimuli were presented close to the patient’s head (20 cm) than when presented far away from it (70 cm). Representations of peripersonal space are body-centred or body part-centred, restricted to the space immediately surrounding the body (extending to about 20–40 cm from the skin surface in monkeys, and up to perhaps 70 cm in humans), and involve the integration of information from multiple sensory modalities (somatosensory, proprioceptive, visual, and auditory). In the course of bodily development, the limbs grow longer, and so become increasingly capable of reaching and grasping nearby objects. In the course of bodily development, the limbs grow longer, and so become increasingly capable of reaching and grasping nearby objects. At the same time, visual stimuli (flashes of light) were presented either next to his real hands (and therefore viewed only in the mirror), or next to the rubber hand illuminated behind the half-silvered mirror. This paper will describe and evaluate the results of recent research on the representation of peripersonal space in both animals and humans. This paper will describe and evaluate the results of recent research on the representation of peripersonal space in both animals and humans. This paper will describe and evaluate the results of recent research on the representation of peripersonal space in both animals and humans. When the visual stimulus was presented next to his real hands and viewed in the mirror, it was effective in extinguishing 33 % of left tactile stimuli. When the visual stimulus was presented next to his real hands and viewed in the mirror, it was effective in extinguishing 33 % of left tactile stimuli. This finding of increased visuotactile extinction when the real hands are viewed in a mirror was examined more thoroughly in a crossmodal congruency task performed by normal human participants (Maravita, Spence, Sergent, & Driver, 2002). Participants were told to ignore the visual distractor stimuli as much as possible, while maintaining central fixation (see Figure 4). Pavani et al  (2000) used the crossmodal congruency task to test the effect of the orientation of rubber arms on visuotactile interference in normal human participants. When we walk hurriedly, with our attention on other things, we might fail to notice certain objects such as a low, overhanging branch crossing our path. Tool-use modifies visuotactile peripersonal space Again, this question has been addressed in several ways; first, in neuropsychological patients with dissociations in neglect for near versus far space, and in crossmodal extinction; and second, in normal human participants using the crossmodal congruency task. Halligan and Marshall asked patients to bisect lines in near and far space. While rightward errors were observed for bisections carried out in near space, no such errors were found for bisections in far space. with severe left visuospatial neglect to bisect lines in near (50 cm from her body) and in far (100 cm) space. This issue of visuospatial versus motor components of neglect needs to be addressed specifically with respect to the use of tools. Left tactile stimuli presented simultaneously with the right visual stimuli were detected on only 53 % of trials immediately following tool-use. Second, and more recently, Maravita, Husain, Clarke, and Driver (2001) introduced several further control conditions to examine the factors contributing to the putative modulation of peripersonal space. Peripersonal right-hand visual stimuli extinguished 94 % of simultaneous left tactile stimuli, while extrapersonal right visual stimulation extinguished only 34 % of simultaneous left touches. Peripersonal right-hand visual stimuli extinguished 94 % of simultaneous left tactile stimuli, while extrapersonal right visual stimulation extinguished only 34 % of simultaneous left touches. Only when the sticks were firmly and actively held in position, connecting the far visual stimulus to the body, was crossmodal extinction of left tactile stimuli significantly increased. The congruency effects for visual stimuli on the opposite side to the tactile stimuli were larger than for same side visual distractors when the tools were crossed. Furthermore, it is of particular interest to us whether tool-use results in an extension of the brain’s representation of the body per se, and/or of the space surrounding the body. Furthermore, it is of particular interest to us whether tool-use results in an extension of the brain’s representation of the body per se, and/or of the space surrounding the body. Each centre processes multisensory information in a reference frame appropriate to the body part concerning which it receives information, and with which responses are to be made. Such knowledge may also enable the design of virtual haptic reality technology (Held & Durlach, 1993), prostheses for amputees (Ramachandran & Rogers-Ramachandran, 1996; Ramahandran, Rogers-Ramachandran, & Cobb, 1995; Sathian, Greenspan, & Wolf, 2000), and navigational aids for the visually impaired (see Hoover, 1950). The neural circuitry of peripersonal space The neural circuitry of peripersonal space For visuotactile peripersonal space and the representation and control of bodily position and movements, a reference frame centred on individual body parts might be the most useful. Mirror neurons are cells that respond to particular goal-directed actions both when the monkey executes that action, and when those same actions are performed, for instance by an experimenter, and observed only passively. Mirror neurons are cells that respond to particular goal-directed actions both when the monkey executes that action, and when those same actions are performed, for instance by an experimenter, and observed only passively. For example, some PMv cells with tactile receptive fields (RFs) on the right arm respond most vigorously to visual stimuli on the right side of space when the arm is held stretched out behind the monkey’s back. Collectively, these results demonstrate that the ventral premotor cortex instantiates a multisensory representation of peripersonal space. Somatosensory impulses project from the thalamus to the primary somatosensory cortices (Brodmann’s areas 1, 2, & 3) in the central sulcus and post-central gyrus. Somatosensory impulses project from the thalamus to the primary somatosensory cortices (Brodmann’s areas 1, 2, & 3) in the central sulcus and post-central gyrus. Area 5 is thought to encode the posture and movement of the body, responding to somatosensory impulses pertaining, for example, to the position of the limbs (Graziano, Cooke, & Taylor, 2000). Area 5 is thought to encode the posture and movement of the body, responding to somatosensory impulses pertaining, for example, to the position of the limbs (Graziano, Cooke, & Taylor, 2000). Area 5 is thought to encode the posture and movement of the body, responding to somatosensory impulses pertaining, for example, to the position of the limbs (Graziano, Cooke, & Taylor, 2000). Area 5 is thought to encode the posture and movement of the body, responding to somatosensory impulses pertaining, for example, to the position of the limbs (Graziano, Cooke, & Taylor, 2000). Area 5 is thought to encode the posture and movement of the body, responding to somatosensory impulses pertaining, for example, to the position of the limbs (Graziano, Cooke, & Taylor, 2000). This finding is important to the present discussion, because another group of researchers have come to quite different conclusions regarding the same area’s involvement in the encoding of multisensory peripersonal space. In their seminal study, Iriki, Tanaka, and Iwamura (1996a) recorded from somatosensory neurons in the anterior bank of the intraparietal sulcus (aIPS, also termed the medial bank, or area PEip, perhaps homologous to area 5 in humans; Rizzolatti, Luppino, & Matelli, 1998). Third, and most importantly, the stimuli used to test the putative visual RFs in Iriki and colleagues’ studies were always presented on a centripetal trajectory approaching and receding from the hand of the monkey (Iriki et al , 1996a; Iriki, Tanaka, Obayashi, & Iwamura, 2001; Obayashi et al , 2000; see Figure 2). In the studies of Fogassi et al  (1996) and Graziano et al  (1997b), formal testing of visual RFs was carried out with robotically-controlled stimuli presented along parallel trajectories and controlled independently from hand, eye, and head position. 