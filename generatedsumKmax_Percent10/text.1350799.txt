Now, if peripersonal space is coded in terms of a bimodal, visuotactile representation, and if a right hemisphere lesion affects this representation, then peripersonal visual stimuli on the right should interfere with the detection of tactile stimuli on the left. When the right visual stimulus was presented far from the hand, or when the patient held their own hand behind their back (see Figure 3), detection of left tactile stimuli improved significantly. Furthermore, when the arms were held in a crossed position (such that the left hand lay in the right hemispace and vice versa for the right hand), visual stimulation near the right hand still induced extinction of left hand tactile stimuli. Stimulation of the same point in space near the hand, when the hand is occluded from view, produces extinction only to the same degree as for visual stimuli presented far from the right hand (e.g., Mattingley et al , 1997).3 Thus, it would appear that proprioceptive signals are insufficient to allow a visual peripersonal stimulus to activate strongly the visuotactile representation of the hand. In summary, combined tactile stimulation of a body part and visual stimulation near that same body part can ameliorate both tactile and visual perceptual deficits. In their patient C.A., right auditory stimulation interfered with the detection of a simultaneous left tactile stimulus (the experimenter tapped the patient’s skin behind her left ear) much more when auditory stimuli were presented close to the patient’s head (20 cm) than when presented far away from it (70 cm). This finding of increased visuotactile extinction when the real hands are viewed in a mirror was examined more thoroughly in a crossmodal congruency task performed by normal human participants (Maravita, Spence, Sergent, & Driver, 2002). Again, this question has been addressed in several ways; first, in neuropsychological patients with dissociations in neglect for near versus far space, and in crossmodal extinction; and second, in normal human participants using the crossmodal congruency task. Halligan and Marshall asked patients to bisect lines in near and far space. with severe left visuospatial neglect to bisect lines in near (50 cm from her body) and in far (100 cm) space. This issue of visuospatial versus motor components of neglect needs to be addressed specifically with respect to the use of tools. Second, and more recently, Maravita, Husain, Clarke, and Driver (2001) introduced several further control conditions to examine the factors contributing to the putative modulation of peripersonal space. Only when the sticks were firmly and actively held in position, connecting the far visual stimulus to the body, was crossmodal extinction of left tactile stimuli significantly increased. The congruency effects for visual stimuli on the opposite side to the tactile stimuli were larger than for same side visual distractors when the tools were crossed. For visuotactile peripersonal space and the representation and control of bodily position and movements, a reference frame centred on individual body parts might be the most useful. For example, some PMv cells with tactile receptive fields (RFs) on the right arm respond most vigorously to visual stimuli on the right side of space when the arm is held stretched out behind the monkey’s back. Collectively, these results demonstrate that the ventral premotor cortex instantiates a multisensory representation of peripersonal space. Somatosensory impulses project from the thalamus to the primary somatosensory cortices (Brodmann’s areas 1, 2, & 3) in the central sulcus and post-central gyrus. Somatosensory impulses project from the thalamus to the primary somatosensory cortices (Brodmann’s areas 1, 2, & 3) in the central sulcus and post-central gyrus. This finding is important to the present discussion, because another group of researchers have come to quite different conclusions regarding the same area’s involvement in the encoding of multisensory peripersonal space. Third, and most importantly, the stimuli used to test the putative visual RFs in Iriki and colleagues’ studies were always presented on a centripetal trajectory approaching and receding from the hand of the monkey (Iriki et al , 1996a; Iriki, Tanaka, Obayashi, & Iwamura, 2001; Obayashi et al , 2000; see Figure 2). In the studies of Fogassi et al  (1996) and Graziano et al  (1997b), formal testing of visual RFs was carried out with robotically-controlled stimuli presented along parallel trajectories and controlled independently from hand, eye, and head position. 