Introduction
Affect processing and the ability to recognize emotions in facial expressions is a crucial part of social cognition and inter-personal relationships. In everyday life, however, the evaluation of facial emotions is rarely based on the expression of a face alone. Rather, most of the time a face is perceived in a particular context, for instance, we see a face in a specific scene or accompanied by a particular sound. Thus, facial affect recognition usually involves the interpretation of a face in a specific context, even if the latter is not processed consciously. Importantly, this context may not always be congruent with the emotional expression displayed by the face, which implies that concurrently receiving two or more different emotional inputs can lead to emotional conflict.
Non-emotional conflict has been studied extensively using behavioural and neuroimaging experiments (Botvinick et al., 2004; Carter et al., 1998; Carter and van Veen, 2007; Durston et al., 2003; Egner and Hirsch, 2005; Kerns et al., 2004; MacDonald et al., 2000; Ochsner et al., 2009; Weissman et al., 2004). The results of these studies have led to the conflict monitoring hypothesis, which suggests that conflict is detected by the dorsal anterior cingulate cortex (ACC), which in turn recruits prefrontal regions to increase cognitive control (Botvinick et al., 2004; Carter and van Veen, 2007; Kerns et al., 2004).
In contrast, rather few studies have focused on the effects of affective conflict, reporting increased reaction times (Collignon et al., 2008; de Gelder and Vroomen, 2000; Dolan et al., 2001; Haas et al., 2006; Ochsner et al., 2009; Wittfoth et al., 2010) in incongruent compared to congruent conditions for both unimodal and crossmodal affective conflict.
On the neurobiological level affective conflict has been linked to activation in ACC for (within-modality) conflicts in the visual and auditory domain (Haas et al., 2006; Ochsner et al., 2009; Wittfoth et al., 2010), as well as right dorsolateral prefrontal cortex (DLPFC) and bilateral posterior medial frontal cortex for visual (Ochsner et al.,2009) and superior temporal gyrus (STG) for auditory conflict processing (Wittfoth et al., 2010).
As outlined above emotional conflicts may arise not only from incompatible information encountered in the same modality but in particular also from receiving non-matching cues from different sensory channels, such as when we see a frightened face and concurrently hear laughter. In spite of their behavioral relevance, however, the neurobiology of multi-modal emotional conflicts has rarely been studied. Studies focusing on the neural correlates of audiovisual integration independent of a potential conflict, consistently report increased activation of posterior superior temporal sulcus (pSTS) (Beauchamp, 2005; Kreifelts et al., 2007; Pourtois et al., 2005; Robins et al., 2009), for both emotional and non-emotional integration. Furthermore Dolan et al. (2001) contrasted emotional congruent to emotional incongruent conditions in an audiovisual paradigm and found greater activation of the left amygdala and right fusiform gyrus (FFG) in congruent as compared to incongruent conditions, but did not report a significant effect of the reverse contrast. Nevertheless studies which focus on the neural correlates of audiovisual emotional incongruence processing are still sparse. Furthermore, most of the existing studies investigating emotional incongruence effects either did not use neutral stimuli or did not differentiate between different ways how incongruence could arise in such settings. In particular, it may be argued that two different types of incongruence can occur in audiovisual pairing when, in addition to emotional, neutral stimuli are also presented. These two types may be called incongruence of emotional valence and incongruence of emotion-presence. On one hand, incongruence of emotional valence arises when in both modalities an emotional stimulus is presented but these two stimuli differ in emotional valence (i.e. happy face paired with scream). That is, the information from the two sensory channels disagrees in the nature or valence of the portrayed emotion. On the other hand incongruence of emotion-presence is characterized by pairing a neutral stimulus with an emotional one (i. e. neutral face paired with a scream). In other words, an emotional stimulus may be present in one modality but absent in the other (because a neutral stimulus is presented). Given that emotion processing is supposed to be centered about filtering behaviorally relevant information, in this case a conflict arises between the two sensory channels, with one indicating a (potentially) salient event, while the other is not.
Finally, clinical studies in patients with depression show that this disorder goes along with a negativity bias (Drevets et al., 2008) and problems inhibiting negative information (Goeleven et al., 2006; Joormann, 2010). These problems have repeatedly been linked to dysregulation in ACC, dorsolateral prefrontal cortex and the amygdala (Abler et al., 2007; Drevets, 2000; Fales et al., 2008). Furthermore symptom severity of depression correlates with amygdala activation in emotion regulation (Abler et al., 2007) as well as inhibitory-related activity in ACC during a cognitive inhibition task (Matthews et al., 2009). Lastly, Abler et al. (2010) demonstrated that activation in the amygdala does even correlate with subclinical depression scores. Whether a similar correlation of depressivity scores with regional brain activation can also be found in crossmodal emotional integration is yet not clarified. Moreover, revealing and understanding the relationship between subclinical traits and neural function in the healthy brain may help to provide information about neurobiological vulnerability markers for depression.
Given the open questions outlined above, the primary goal of the current study is to investigate the influence of emotional sounds on facial affect perception and the underlying neural substrates involved in the mediation of both types of audiovisual (in-) congruence effects. We hypothesize that behaviorally, emotional sounds influence the perception of facial expressions while incongruence effects should elicit increased activation in dACC and prefrontal regions. In contrast, based on previous work, we would assume that emotional congruence enhances activation in amygdala and FFG.
As a secondary aim the correlation of (subclinical) depressive traits as quantified by the Beck Depression Inventory (BDI) with the neural correlates of audiovisual inhibition processes is investigated. Given previous reports in depressed patients (Abler et al., 2007; Drevets, 2000; Fales et al., 2008), we would hypothesize that activity in the amygdala, ACC and DLPFC over all task conditions relative to baseline correlates with BDI scores.
Methods
SubjectsIn total 40 healthy subjects were examined. Five subjects were excluded from the analysis due to BDI or alexithymia scores exceeding the cut-off for clinically relevant depression (> 15) and alexithymia (>60), respectively, or excessive movement during scanning. Hence, 35 healthy right-handed volunteers (20 females; mean age 29.4 ± 8.1 years, mean education 14.1 ± 3.2, 15 males; mean age 34.0 ± 13.3, mean education 14.5 ± 3.2) were included in the analysis. Men and women did not significantly differ in age (t33 = 1.29, ns.) or education (t33 = 0.34, ns.). All participants had normal or corrected-to-normal vision and were right handed as confirmed by the Edinburgh Inventory (Oldfield, 1971). The structured clinical interview of DSM IV (SCID; Wittchen et al., 1997) was used to screen the subjects revealing no history of neurological or psychiatric disorders including substance abuse. No subject took mood- or cognition-altering medication. Subjects gave informed consent into this study which was approved by the ethics committee of the School of Medicine of the RWTH Aachen University.
StimuliThe visual stimuli, obtained from the FEBA inventory (Gur et al., 2002a), consisted of color pictures of male and female faces, showing either a happy, neutral or fearful expression. In total 10 (5 males and 5 females) faces of different actors each showing every expression (happy, neutral, fear) were used. To optimize the design, a behavioral pre-study with 48 subjects was conducted where different versions of the paradigm and varying presentation times were tested. These results indicated that emotional faces were too clear to allow any consistent context framing effects. As a consequence, happy and fearful faces were degraded in emotionality in order to make them more ambiguous. This was done by merging every emotional facial image with the neutral mouths of the same actor.Furthermore, as a longer auditory and a shorter visual stimulus had to be combined, each trial also employed (visual) masks. This was necessary because a long auditory presentation is needed to contain clear emotional information, whereas a short visual presentation is important to allow context effects. As visual input should be held constant for the whole trial 10 different neutral faces blurred with a mosaic filter were included as masks.The auditory stimuli were either neutral (yawning) or socio-emotional salient sounds like laughing (happy) or screaming (fear). 210 different sounds were obtained from various sound libraries and equalized with respect to length and volume. In a pre-experiment, these were then rated by a different sample of 10 subjects for validation and selection of the stimuli used in the experiment. In total 10 laughs, 10 yawns and 10 screams (5 males and 5 females in each case) were selected.
ProcedureIn the experiment a total of 180 stimulus pairs, each consisting of a visual and an auditory stimulus were presented. Every face condition (happy, fear, neutral) was combined with every sound condition (happy, fear, neutral) resulting in a 3×3 design with nine different conditions and 20 audiovisual pairs per condition. The pairing was pseudo-random and matched with regard to gender, so that a male face was always paired with a male voice and vice versa.Audiovisual stimulus pairs were presented for 1500 ms with a jittered inter-stimulus-interval between 4000 and 6000 ms during which a blank black screen was shown (see Fig. 1). Every trial started with the presentation of a sound to establish a context. Concurrently with the sound a blurred neutral face was shown. After 1000 ms the blurred face was displaced by the target stimulus (neutral or emotional face) and presented with the continuing sound for another 500 ms. The subjects were asked to ignore the sound and to rate as fast and accurate as possible the presented facial expression on an eight-point rating scale from extremely fearful to extremely happy. Subjects were instructed that the study focuses on attention processes in order to avoid effects of expectation due to knowledge about the aims of the study. Stimuli were presented with the software Presentation 14.2 (http://www.neurobs.com/) and response was given manually with the right or left hand on an MR-compatible response pad (from extremely fearful—left little finger (1) to extremely happy—right little finger (8)).
Additional tests and questionnairesAfter the scanning session participants rated the valence and arousal of sounds and pictures individually on a nine-point valence scale ranging from very fearful to very happy and not at all arousing to very arousing, respectively. Contrary to the rating in the scanner where subjects had to make a forced choice on the emotional content due to the absence of a “neutral” response, the rating of faces and sounds after scanning included a neutral category. Moreover the TAS-D (Bagby et al., 1994) was carried out to control for alexithymia and the Beck Depression Inventory (BDI-II; Hautzinger et al., 2006) to check for subclinical depressive symptoms.
fMRI data acquisitionImages were acquired on a Siemens Trio 3 T whole-body scanner (Erlangen, Germany) in the Research Center Jülich using blood-oxygen-level-dependent (BOLD) contrast (Gradient-echo EPI pulse sequence, TR = 2.2 s, in plane resolution = 3.1 × 3.1 mm, 36 axial slices (3.1 mm thickness)) covering the entire brain. Image acquisition was preceded by 4 dummy images allowing for magnetic field saturation. These were discharged prior to further processing. Images were analysed using SPM5 (www.fil.ion.ucl.ac.uk/spm). First, the EPI images were corrected for head movement by affine registration using a two-pass procedure, by which images were initially realigned to the first image and subsequently to the mean of the realigned images. After realignment, the mean EPI image for each subject was spatially normalized to the MNI single subject template using the “unified segmentation” approach (Ashburner and Friston, 2005). The resulting parameters of a discrete cosine transform, which define the deformation field necessary to move the subjects data into the space of the MNI tissue probability maps, were then combined with the deformation field transforming between the latter and the MNI single subject template. The ensuing deformation was subsequently applied to the individual EPI volumes that were hereby transformed into the MNI single subject space and resampled at 2×2×2 mm3 voxel size. The normalized images were spatially smoothed using an 8 mm FWHM Gaussian kernel to meet the statistical requirements of the General Linear Model and to compensate for residual macro-anatomical variations across subjects.
Statistical analysisBehavioral data were analyzed off-line using SPSS 18.0.0 (SPSS Inc., Chicago, IL). All data were confirmed to be normally distributed and MANOVAs / ANOVAs were calculated. Violations of sphericity were corrected by the Greenhouse–Geisser or Huynh–Feldt correction. Post-hoc analyses, corrected for multiple comparisons, were calculated for significant main effects and interactions.The imaging data were analyzed using a General Linear Model as implemented in SPM5. Each experimental condition (fearful face/scream, fearful face/yawn, fearful face/laugh, neutral face/scream, neutral face/yawn, neutral face/laugh, happy face/scream, happy face/yawn, happy face/laugh) as well as the response event were separately modelled by a boxcar reference vector convolved with a canonical hemodynamic response function and its first-order temporal derivative. Additionally, low-frequency signal drifts were filtered using a cutoff period of 128 s. Parameter estimates were subsequently calculated for each voxel using weighted least squares to provide maximum likelihood estimators based on the temporal autocorrelation of the data (Kiebel et al., 2003). No global scaling was applied. For each subject, simple main effects for each of the nine experimental conditions and the response were computed by applying appropriate baseline contrasts. These individual first-level contrasts were then fed to a second-level group-analysis using an ANOVA (factor: condition, blocking factor subject) employing a random-effects model. In the modelling of variance components, violations of sphericity have been allowed by modelling non-independence across images from the same subject and allowing unequal variances between conditions and subjects using the standard implementation in SPM 5.In the ANOVA mean (weighted by non-sphericity correction) parameter estimates over all subjects were calculated for every condition (fearful face/scream, fearful face/yawn, fearful face/laugh, neutral face/scream, neutral face/yawn, neutral face/laugh, happy face/scream, happy face/yawn, happy face/laugh). Based on these estimates of the second level analysis, separate t-contrasts were calculated for every differential effect (effect of fearful faces, effect of happy faces, effect of fearful sounds, effect of happy sounds, incongruence effects of emotional valence, incongruence effects of emotion-presence) by applying the respective contrast to the 2nd level parameter estimates. In other words, the ANOVA was used to calculate the group level parameter estimates for each condition whereas t-contrasts to the parameter estimates were employed to test for differential effects between these. The resulting SPM (T) maps were then thresholded at p <0.05, correcting for multiple comparisons by controlling the family-wise error (FWE) rate according to the theory of Gaussian random fields (Worsley et al., 1996). FWE corrects for accumulation of type I error in multiple testing with non-compact support, i.e., correlated statistics. By regarding the statistical image as a lattice representation of a random Gaussian field (whose smoothness is estimated using the residual field of the statistical analysis), the probability that one or more false positive voxels are found in the entire search volume can be estimated for each threshold applied to the statistic. The threshold is then chosen as to control the family-wise error rate at p <0.05, i.e., in only one of twenty analyses of random fields of the same type, size and smoothness one would expect to find a single voxel above the chosen threshold.Additionally, another model was estimated where the nine conditions and responses and BDI-scores as a covariate were modeled.Ensuing activations were anatomically localised using version 1.6b of the SPM Anatomy toolbox (Eickhoff et al., 2005, 2006, 2007; www.fz-juelich.de/ime/spm_anatomy_toolbox).
Results
Valence and arousal of faces and sounds in the off-line ratingAn one-factorial MANOVA with the factor emotion (fear/neutral/happy) and the dependent variables “off-line valence rating of faces” and “off-line valence rating of sounds” was conducted revealing a significant main effect of emotion (F2,4 = 318.11, p < 0.05). This effect was significant for both dependent variables, faces (Greenhouse–Geisser corrected: F2,66 = 271.2; ε =0.74, p < 0.05) and sounds (Greenhouse–Geisser corrected: F2,66 = 983.42, ε = 0.76, p < 0.05). Fearful stimuli got the lowest ratings, followed by neutral faces and sounds, while the most positive ratings were given for happy stimuli (all post-hoc comparisons were significant at p < 0.05, cf. Table 1).An additional MANOVA with ratings of arousal also led to a significant main effect of emotion (F2,4 = 53.14, p<0.05), significant for both stimulus modalities (faces: F2,68 = 13.72; p<0.05; sounds: F2,68 = 95.95; p<0.05). Neutral faces and sounds were rated as least arousing, followed by happy stimuli, whereas fearful stimuli were perceived as most arousing (all post-hoc comparisons were significant at p<0.05 except for comparison of happy to neutral faces which revealed a tendency towards significance, cf. Table 1).
On-line behavioral dataAn ANOVA with the factors face (fear/neutral/happy) and sound (scream/yawn/laugh) and the dependent variable “rating obtained in the scanner” revealed significant main effects of face (Huynh–Feldt corrected: F2,68 = 253.96, ε = 0.66, p<0.01) and sound (F2,68 = 7.96, p<0.01), as well as a significant interaction between face x sound (F4,136 = 3.86, p<0.01).Post-hoc analyses of the main effect face showed the same pattern as observed in the off-line rating, with lowest ratings for fearful, highest for happy and intermediate values for neutral faces (all post-hoc comparisons were significant at p<0.05).As the task was to ignore the sound and to rate the face, the main effect of sound reflects the influence of a sound on a face. Post-hoc analyses of this effect did show that screams led, independently of facial expression, to a more fearful rating of the faces compared to yawns and laughs (post-hoc comparisons were significant at p<0.05), whereas subjects did not rate faces differently when exposed to laughs compared to yawns.Post-hoc analyses of the interaction revealed that subjects rated fearful faces as being more fearful when accompanied by screams compared to yawns or laughs. Neutral faces were also perceived as more fearful when paired with screams compared to yawns but not to laughs (Fig. 2). No context effect could be found for happy faces (Tukey's HSD=0.096).
Imaging dataTo identify emotion specific areas all happy and fearful faces were separately contrasted to neutral faces. For happy relative to neutral faces, this contrast revealed increased activation in bilateral calcarine sulcus, left middle occipital gyrus, left inferior parietal cortex, left supplementary motor area, primary motor and postcentral gyrus as well as left insula and right thalamus (p<0.05, FWE corrected, Table 2). In contrast, there was no significant greater activation for fearful compared to neutral faces at the corrected level. Comparison of screams and laughs, respectively, to yawns showed activation in bilateral STG for either emotional sound. Furthermore screams compared to yawns activated bilateral inferior parietal cortex (IPC), right supplementary motor area (SMA) and left middle cingulate cortex (p<0.05, FWE corrected, Table 3).
Incongruence effects of emotional valenceTo identify brain regions involved in detecting and processing emotional conflict of valence, emotionally incongruent conditions (fearful face paired with laugh and happy face paired with scream) were contrasted to emotionally congruent conditions (fearful face paired with scream and happy face paired with laugh). Significant incongruence effects could be found in the middle cingulate cortex, the right superior frontal gyrus, the right SMA and the right temporoparietal junction (TPJ) (p<0.05, FWE corrected, Table 4, Fig. 3).In contrast, congruent compared to incongruent conditions did not evoke significantly increased activation in any brain region. As neither incongruence nor congruence effects were found in the amygdala, a small volume correction (based on the coordinates by Dolan et al., 2001; x=−20, y=−8, z=−14 Talairach coordinates converted into MNI space x=−20, y=−8, z=−17; search volume 6 mm) was carried out for both congruence and incongruence effects. Again, no significant effect was detected, neither FWE corrected, nor uncorrected.
Incongruence effects of emotion-presenceTo elucidate the neural correlates of incongruence concerning the presence and absence of emotion, we compared pure emotional conditions (fearful face with laugh or scream, happy face with laugh or scream) with conditions where a neutral stimulus was presented in one of the two modalities and an emotional one in the other (fearful or happy face with yawn, neutral face with scream or laugh).This contrast revealed significant activation (cluster peak at x=−20, y=−3, z=−21, p<0.05, FWE corrected, Fig. 4) of the left laterobasal/superficial nuclei of the amygdala (LB/SF; Amunts et al., 2005). The overlap of the activated cluster with LB was 15.2%, while 78.6 % of its volume was allocated to SF; the maximum probabilities were 60 % and 70 %, respectively.Furthermore this contrast led to significant greater bilateral occipital and temporal activation (Table 5).
Correlation with BDi-scoresAs it is known that depression goes along with impairments in emotional processing, we were interested if even subclinical depressive symptoms correlate with neural activation in an emotion-related task.An analysis with BDI-Scores as covariate revealed a negative correlation of BDI with activation (across all conditions) bilaterally in middle occipital gyrus (x=−38; y=−86; z=9 and x=33; y=−89; z=8) and in right pSTS (x=45; y=−44; z=8) and FFG (x=29; y=−60; z=−9, all cluster-level corrected p<0.05) (Fig. 5). There was no significant interaction between BDI and condition.
Discussion
By using fMRI we investigated the influence of emotional sounds on the perception of facial expressions and the neural correlates of incongruence processing. The results revealed an influence of screams on the perception of fearful and neutral faces and incongruence effects of emotional valence were found in middle cingulate cortex, right superior frontal gyrus, right supplementary motor area (SMA) and right temporoparietal junction (TPJ). Furthermore activation in the left amygdala was attenuated when an emotional stimulus was paired with a neutral one compared to when in both modalities the stimulus was emotional.
Off-line rating of faces and sounds and neural correlates of face and sound processingThe off-line rating of faces and sounds showed that both, faces and sounds were recognized correctly by the subjects with lowest ratings for fearful faces and sounds, followed by neutral stimuli and the highest (happiest) ratings for happy faces and sounds. Additionally the rating of faces reflect that the perceived emotion was more intense in happy as compared to fearful faces, evident by a larger difference to the neutral category.This pattern is reflected in the imaging data. While happy compared to neutral faces activate a widespread cortical network, no significantly (at the corrected level) increased activity was observed for fearful as compared to neutral faces. Increased activation in occipital areas (Britton et al., 2006), insula (Britton et al., 2006; Chen et al., 2009) and thalamus (Dolan et al., 1996) for happy faces are well in accordance with previous results.In contrast, on the FWE correction level, fearful faces did not recruit more activation in any areas compared to neutral faces. This reflects the relative moderate expression of fear in the fearful face condition and goes along with the less intense rating of these faces after scanning.Comparing laughs and screams to yawns activated bilateral superior temporal gyrus (STG), which is in line with previous findings on emotional vs. neutral vocalizations (Fecteau et al., 2007; Meyer et al., 2005; Phillips et al., 1998). As Fecteau et al. (2007) pointed out this activation may either reflect differences in acoustic characteristics or indicate that the STG plays a role in extracting emotional information from socially relevant sounds. Moreover, screams compared to yawns led to greater activation in bilateral inferior parietal cortex (1PC), right SMA and left middle cingulate cortex. This pattern may reflect that the higher arousal evoked by the screams (as reflected by the off-line ratings) has led to activation of parietal areas to augment attention (Corbetta and Shulman, 2002), cingulate cortex for monitoring (Botvinick et al., 2004) and SMA for increased motor control (for review see Nachev et al., 2008).
Context effects on ratings of facial affectOnly two studies (de Gelder and Vroomen, 2000; Ethofer et al., 2006) previously investigated how emotional sounds influence the perception of (emotional) facial expressions. Our behavioral data obtained in the scanner demonstrated that context, established by concurrent social–emotional sounds, influences the perception of facial expressions. Fearful and neutral faces were perceived more fearful when accompanied by screams compared to yawns (or laughs in case of fearful faces). In contrast, these context effects were not found for happy faces. This is in line with the results of Ethofer et al. (2006) who also report context effects for fearful and neutral but not for happy faces. One explanation for the lack of effect on happy faces may be differences in ambiguity, as indicated by the off-line ratings. This difference occurred even though happy and fearful faces were matched in intensity based on the classification of Gur et al. (2002a), were degraded by the neutral mouth and showed equally clear separation from neutral faces in a prestudy conducted in a different sample of 18 subjects. These results thus support earlier notions that happiness is the easiest and fear one of the hardest recognizable emotions in faces (Gur et al., 2002a; Montagne et al., 2007).Compared to yawns only screams, not laughs led to a significantly more emotional rating, potentially relating to the higher arousal evoked by these sounds which makes them much harder to ignore. This higher saliency fits well with the biological perspective that screams, as threat signals, are far more important for survival than laughs (Calder et al., 2001; Green and Phillips, 2004) and is further-more reflected by the more widespread activation evoked by these (as described above).
Neuronal effects of incongruence of emotional valenceEmotional incongruence or conflict (pairing of fearful faces with laughs and happy faces with screams) evoked greater activation in middle cingulate cortex, right superior frontal gyrus, right SMA and right TPJ compared to emotional congruent trials. Crossmodal emotional conflict thus recruits a network similar to those previously described to be involved in unimodal emotional and cognitive conflict and attentional control (Botvinick et al., 2004; Corbetta et al., 2008; Ochsner et al., 2009; Wittfoth et al., 2010).The (anterior) cingulate cortex (ACC) has repeatedly been linked to conflict monitoring (Botvinick et al., 2004; Carter et al., 1998; Carter and van Veen, 2007) and has been found by Ochsner et al. (2009) and Egner et al. (2008) for cognitive and affective conflict in the visual domain and by Wittfoth et al. (2010) for auditory emotional conflict. It should be noted, that the location of these findings, although reported as “ACC” corresponds well to the increased activation observed in the middle cingulate cortex in our study. This discrepancy should be attributable to the still widespread use of the outdated two-region classification of the cingulate cortex introduced by Brodmann in spite of convincing evidence for at least four major regions of this structure (Palomero-Gallagher et al., 2009; Vogt et al., 2004). As this region has already been demonstrated to respond to unimodal (Egner et al., 2008; Ochsner et al., 2009; Wittfoth et al., 2010) and non-emotional audiovisual (Weissman et al., 2004) conflict, current findings in emotional audiovisual conflict provide further evidence for a generalized role of the dorsal middle cingulate cortex in conflict monitoring and attentional control, irrespectively of modality, integration demands or affective content. Similarly, several studies have already linked the superior frontal / dorsolateral prefrontal cortices to the resolution of (non-emotional) conflict (Corbetta and Shulman, 2002; Durston et al., 2003; Egner and Hirsch, 2005; MacDonald et al., 2000) mainly by top-down control of attentional resources. Again, our results extend these previous findings to emotional conflict, pointing to an overlapping neuronal framework.The SMA plays a major role in voluntary action, cognitive control and initiation/inhibition of motor responses (Grefkes et al., 2008; Kasess et al., 2008; Nachev et al., 2008; Sumner et al., 2007). SMA activation was also found for emotional conflict in the study of Ochsner et al. (2009) in an affective flanker task. Like these authors we would conclude that higher SMA activity may reflect increased executive control needed to select an adequate response in the presence of conflicting (emotional) stimuli.The TPJ, finally, has been implicated in predictive coding (Jakobs et al., 2009), attentional reorienting, theory of mind and perspective taking tasks (Corbetta et al., 2008; Derntl et al., 2010; Mitchell, 2008). In our current data, we would side with the idea of predictive coding postulating that the brain predicts upcoming events based on previous information in order to reduce computational load and to deal with ambiguous information (Creutzig and Sprekeler, 2008). The auditory stimulus, starting 1000 ms before the face, could have induced expectation of a visual stimulus congruent with the emotion expressed in the sound. The presentation of an emotionally incongruent face should then have led to violation of these expectations. Consequently, if the TPJ integrates sensory signals and against the expectations on these, the prediction error between bottom-up facial information and expectation built on the sound may underlie the greater TPJ activation in the incongruent condition.In summary, audiovisual emotional incongruence leads to activation of a cingulate-fronto-parietal network previously shown to be involved in conflict monitoring and resolution (Botvinick et al., 2004; Carter et al., 1998; Ochsner et al., 2009; Weissman et al., 2004; Wittfoth et al., 2010). Conflict caused by the convergence of two incongruent information streams is detected by the dACC/MCC (Botvinick et al., 2004; Carter et al., 1998; Carter and van Veen, 2007; Kerns et al., 2004), which recruits the dorsal prefrontal cortex and the SMA to increase attentional and motor control to enable selection of an adequate response. The current results expand this conflict monitoring hypothesis in two ways. Firstly, we would argue for a role of the TPJ in processing temporally asynchronous conflicting information by integrating predictions and incoming stimuli (Jakobs et al., 2009). Second, our results indicate that the same conflict monitoring and resolution network demonstrated for cognitive and action related tasks, is likewise involved in emotional conflict processing.This suggestion is in line with the biased competition model of attention introduced by Desimone and Duncan (1995), postulating that competition for neural representation occurs when two or more stimuli with different task relevance are presented. This competition can in turn be modulated (biased) in two different ways, namely either based on bottom-up (salience) or by top-down (cognitive control) influences. The increased activity of cingulate, frontal and parietal areas in incongruent compared to congruent emotional conditions observed in our study may reflect such top-down mechanisms. In particular, given that subjects were instructed to attend to and rate the faces rather than the sounds, these regions may be the origin of top-down signals biasing competition towards the task relevant stimuli (faces) in incongruent conditions where competition and attentional demands are higher. In line with this suggestion, other studies also demonstrate greater activity in ACC (Bishop et al., 2004a) and parietal and frontal (Mitchell et al., 2007) areas going along with higher attentional demands when a cognitive task is distracted by emotional stimuli. Furthermore it is reported that attention towards a non-emotional task suppresses activity in the amygdala (Amting et al., 2009; Bishop et al., 2004b; Mitchell et al., 2008; Pessoa et al., 2005). The absence of a (valence) congruency effect in the amygdala could therefore just be the result of biased competition, as the salience of emotional sounds is likewise suppressed in favor of the (attended) face.
Role of the amygdalaIn contrast to the finding of Dolan et al. (2001) we did not observe any (in-) congruence effects of emotional valence in the amygdala when explicitly testing for these. However, the testing of incongruence effects with respect to the presence and absence of emotional stimuli (incongruence of emotion-presence) revealed a significant effect in the left basolateral/superficial amygdala complex.It should be noted that our result should be interpreted with caution given the current resolution of fMRI and the susceptibility of the medial temporal lobe to imaging artifacts. Nevertheless, the applied histological atlas allows anatomical allocation in a probabilistic fashion, hereby accommodating some of the uncertainty in establishment of structure-function relationships (cf. Amunts et al., 2005; Eickhoff et al., 2005). Moreover, this approach has already been repeatedly shown to allow the attribution of functional activation to specific parts of the amygdala (Ball et al., 2007; Goossens et al., 2009; Roy et al., 2009).It should also be taken into account that we used a somewhat larger smoothing kernel as previous studies which targeted particularly at amygdala responses (Ball et al., 2007; Goossens et al., 2009; Roy et al., 2009). In this context, however, it is important to note, that while smoothing blurs the data, it should per se not affect localisation accuracy (Fox et al., 2001). Rather spatial uncertainty in neuroimaging results mainly derives from inter-individual variability and imprecision in spatial normalisation (Eickhoff et al., 2009; Grachev et al., 1999; Hellier et al., 2003). To minimise the latter potential confound, we used the individual subjects’ mean EPI image directly as the source image for spatial normalisation rather than co-registering T1. This approach has the advantage of avoiding influences of non-linear distortions between EPI and T1 images and yields a precise registration of the functional data into MNI space (see supplementary Fig. 1).Though, our result indicates that in audiovisual pairing the left amygdala is active independent of crossmodal valence congruency when emotional information is not accompanied by concurrent neutral information. Conversely, it may be hypothesized, that a neutral stimulus in one sensory channel may counteract the emotional saliency provided by the stimulus in the other modality by attenuating amygdala responses. In other words, in audiovisual integration it is not only the presence of an emotional stimulus but rather the absence of a concurrent, mitigating neutral stimulus that may be crucial for left amygdala responses. Could this effect be related to priming, so that the presentation of a neutral stimulus influences amygdala response to the subsequent facial stimulus? Importantly this is not the case, as the pattern of activation is the same independently of whether the neutral stimulus (yawn) was presented first or as a neutral face 1000 ms after the onset of an (emotional) sound.The current findings thus place a new twist on the current views about the function and role of the left amygdala. It has previously been demonstrated that this structure is activated by attended and unattended emotional stimuli in the visual or auditory modality (Belin et al., 2004; Fecteau et al., 2007; Gur et al., 2002b; Habel et al., 2007; Phillips et al., 1998; Sander and Scheich, 2001). We could show that the left laterobasal/superficial amygdala nuclei also integrate neutral stimuli across modalities, which probably leads to an “all-clear” signal. From studies in macaque monkeys it is known that the laterobasal amygdala receives direct input from auditory and visual cortices (McDonald, 1998; Yukie, 2002). Moreover feedback projections from the basal nucleus of the amygdala to sensory-associative areas have been demonstrated (Amaral et al., 2003; Yukie, 2002). This connectivity pattern in combination with the observed response pattern may be interpreted as follows. The left amygdala receives socially salient information from all sensory domains which are then integrated and evaluated. Information processing in visual and auditory areas is then modulated via the feedback projections from the laterobasal amygdala to early sensory cortices providing a route for enhancing behaviourally relevant and suppressing less important information. Ultimately, integrating a neutral stimulus into the evaluation by the amygdala should then lead to top-down inhibition in associative visual and auditory areas whereas emotional stimuli in both modalities induce top-down facilitation by the amygdala as evident from significantly higher activation in the purely emotional conditions in lingual gyrus, calcarine sulcus, STG and middle temporal gyrus (MTG).
Correlation with BDIBDI scores were negatively correlated with activation in left and right occipital gyrus as well as right FFG and STS, i.e., regions contributing to the neural system for facial perception (Haxby et al., 2000). Moreover, the posterior superior temporal sulcus plays also a crucial role in audiovisual integration (Beauchamp, 2005; Campanella and Belin, 2007). As we only investigated healthy subjects with BDI scores in the normal range, these scores may be regarded as subclinical depressive symptoms or traits which nevertheless were shown to correlate with neuronal activation in an emotion-related task. The observed correlation in sensory cortices may point to a dysregulation of neuronal activity by depressive traits spreading beyond areas involved in emotion regulation (for a review on findings in depression see Leppanen, 2006) to early stages of face processing. Alternatively, the negative correlation of BDI scores and activity in STS could indicate decreased crossmodal integration associated with higher depressivity, which has yet to be tested in patients with major depression. In particular, it remains to be seen, how evaluation in the amygdala and integration in the STS may be differentially affected in these patients.
ConclusionOur audiovisual integration paradigm demonstrated that crossmodal emotional conflict recruits a cingulate-fronto-parietal network previously shown for cognitive, motor and (unimodal) emotional conflict monitoring and resolution. We propose that emotional conflict is detected by the TPJ integrating bottom-up input and expectation while the middle cingulate cortex recruits the dorsal prefrontal cortex and SMA to increase cognitive and motor control for selecting the adequate response.Additionally, our results reveal that irrespective of congruency the left laterobasal/superficial amygdala is attenuated as soon as one sensory channel carries neutral but ecologically valid information. This indicates that crossmodal evaluation in the amygdala integrates emotional as well as neutral information with the latter providing a “clear-off” for implicit arousal regulation.At last, BDI scores correlated negatively with activation in occipital gyrus, FFG and pSTS. These results emphasize the importance of taking even subclinical traits into account when investigating emotional processing.
Supplementary Material
supp
